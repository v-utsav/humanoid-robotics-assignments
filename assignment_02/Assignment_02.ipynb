{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02 for 3D World Representations\n",
    "\n",
    "In this assignment you will complete two programming tasks and answer 1 written question. You are expected to use Python along with libraries such as **NumPy**, **SciPy**, **Matplotlib**, and **Open3D** (for 3D visualization). Please ensure your code is well-commented and modular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf75d3",
   "metadata": {},
   "source": [
    "\n",
    "## 1 Task Based Mapping Methods\n",
    "Roboticists at the University of Bonn are developing several robots for different applications: \n",
    "\n",
    "- **UBoButler**: A humanoid robot tasked with sorting objects on a kitchen counter. The robot must identify and manipulate items like 'cups', 'plates', and 'utensils' and place them in designated locations. It does not wander out of the kitchen.\n",
    "\n",
    "- **UBoWanderDog**: UBoWanderDog: UBoWanderDog is a quadruped navigating a hilly terrain with obstacles such as 'rocks', 'trees', and 'bushes'. The robot's primary task is to explore the area and map potential paths.\n",
    "\n",
    "- **UBoAssist**: UBoAssist is a humanoid assisting elderly and visually challenged people in navigation in dense urban areas with pedestrians, cyclists, other vehicles, traffic lights, and crosswalks.\n",
    "\n",
    "These robots rely on accurate 3D perception to function effectively. Their perception systems process visual data to create 3D maps of their environments. The team is focused on using pixel-level classification from deep learning models to label elements in the scene. To understand the challenges involved, consider the following computational cost considerations:\n",
    "\n",
    "- **Panoptic Segmentation**: Highest computational cost.\n",
    "\n",
    "- **Semantic Segmentation**: Provides pixel-level classification of object categories\n",
    "\n",
    "- **Instance Segmentation**: Similar to Semantic Segmentation but requires additional computation for tracking individual instances.\n",
    "\n",
    "Determine the most suitable scene understanding approach for mapping (Semantic Segmentation, Instance Segmentation, or Panoptic Segmentation) for each of the three scenarios (Kitchen, Hilly Terrain, and Downtown Area). Justify your choice for each scenario. \n",
    "\n",
    "***Answer here in plain text***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Semantic Point Cloud Creation and Stitching            \n",
    "\n",
    "In this task you are provided with a set of depth images, semantic segmentation masks, camera poses, and intrinsics. Your goal is to generate and visualize a colored 3D point cloud. Each depth image should be converted to a local point cloud, colored using the semantic mask and a provided CSV file that maps 41 semantic classes (including background) to RGB colors, and then transformed into world coordinates using the camera pose. Finally, the per-image point clouds should be stitched together and visualized as one global colored point cloud.\n",
    "\n",
    "- a. Convert each depth image into a 3D point cloud using the camera intrinsics. \n",
    "\n",
    "- b. Modify above function to color the point cloud. For each depth, use the corresponding semantic segmentation mask and the provided CSV file mapping semantic classes to colors, to assign an RGB color to each point. Use nearest point interpolation for aligning depth image with segmentation image. Assume segmentation image has same instrinsics as color image. Assume the optical centers for RGB and depth images are co-incident. \n",
    "\n",
    "- c. Then, transform the colored local point cloud into world coordinates using the provided camera pose.\n",
    "\n",
    "- d. Stitch the transformed point clouds from all images into a single global point cloud and visualize it using a 3D visualization tool such as Open3D. \n",
    "- e. Explain in text the trade-off between occupancy and TSDF maps on one hand and different grid sizes on the other based on the visualizations generated\n",
    "\n",
    "### Note:\n",
    "- Use appropriate libraries (e.g., Open3D) for point cloud creation and visualization.\n",
    "- Make sure to apply the correct transformations using camera poses and intrinsics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "72785310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import open3d as o3d\n",
    "\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7517b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    ## Load all the depth images, semantic mask images, camera poses, intrinsics and csv for mapping semantic classes to colors\n",
    "    #pass\n",
    "    color_imgs = []\n",
    "    depth_imgs = []\n",
    "    label_imgs = []\n",
    "    pose_matrices = []\n",
    "    class_map = {}\n",
    "    intrinsic_color = []\n",
    "    intrinsic_depth = []\n",
    "    \n",
    "    path = '../assignment_02/task_2/'\n",
    "\n",
    "    for img in os.listdir(path + 'color/'):\n",
    "        color_imgs.append(matplotlib.image.imread(path + 'color/' + img))\n",
    "\n",
    "    for img in os.listdir(path + 'depth/'):\n",
    "        depth_imgs.append(matplotlib.image.imread(path + 'depth/' + img))\n",
    "\n",
    "    for img in os.listdir(path + 'label/'):\n",
    "        label_imgs.append(matplotlib.image.imread(path + 'label/' + img))\n",
    "\n",
    "    for txt in os.listdir(path + 'pose/'):\n",
    "        with open(path + 'pose/' + txt, 'r') as file:\n",
    "            matrix = [line.strip().split() for line in file]\n",
    "            matrix = np.array(matrix, float)\n",
    "        pose_matrices.append(matrix)\n",
    "\n",
    "    class_map = pd.read_csv('../assignment_02/task_2/class_rgb_map.csv').to_dict()\n",
    "\n",
    "    with open(path + 'intrinsics_color.txt', 'r') as file:\n",
    "        intrinsic_color = [line.strip().split() for line in file]\n",
    "        intrinsic_color = np.array(intrinsic_color, float)\n",
    "\n",
    "    with open(path + 'intrinsics_depth.txt', 'r') as file:\n",
    "        intrinsic_depth = [line.strip().split() for line in file]\n",
    "        intrinsic_depth = np.array(intrinsic_depth, float)\n",
    "        \n",
    "    return color_imgs, depth_imgs, label_imgs, pose_matrices, class_map, intrinsic_color, intrinsic_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5d466",
   "metadata": {},
   "source": [
    "# a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "987dcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_depth_to_pointcloud(depth, depth_intrinsics):\n",
    "    #pass\n",
    "    # getting the data of the depth image and intrinsics\n",
    "    h, w = depth.shape\n",
    "    fx = depth_intrinsics[0][0]\n",
    "    fy = depth_intrinsics[1][1]\n",
    "    cx = depth_intrinsics[0][2]\n",
    "    cy = depth_intrinsics[1][2]\n",
    "\n",
    "    point_cloud = []\n",
    "\n",
    "    # going through the image depth \n",
    "    for u in range(w):\n",
    "        for v in range(h):\n",
    "            # checking the depth value, if its non zero, make the pointcloud using intrinsics\n",
    "            z = depth[v][u]\n",
    "            if z > 0:\n",
    "                x = z*(u - cx)/fx\n",
    "                y = z*(v - cy)/fy\n",
    "                point_cloud.append([x, y, z])\n",
    "\n",
    "    return np.array(point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786188cb",
   "metadata": {},
   "source": [
    "# b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "813369ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_depth_semantics_to_colored_pointcloud(depth, semantics, depth_intrinsics, color_intrinsics):\n",
    "    #pass\n",
    "    # getting the data of the depth image and intrinsics\n",
    "    h, w = depth.shape\n",
    "    fx = depth_intrinsics[0][0]\n",
    "    fy = depth_intrinsics[1][1]\n",
    "    cx = depth_intrinsics[0][2]\n",
    "    cy = depth_intrinsics[1][2]\n",
    "\n",
    "    fx_s = color_intrinsics[0][0]\n",
    "    fy_s = color_intrinsics[1][1]\n",
    "    cx_s = color_intrinsics[0][2]\n",
    "    cy_s = color_intrinsics[1][2]\n",
    "\n",
    "    point_cloud = []\n",
    "\n",
    "    # going through the image depth \n",
    "    for u in range(w):\n",
    "        for v in range(h):\n",
    "            # checking the depth value, if its non zero, make the pointcloud using intrinsics\n",
    "            z = depth[v][u]\n",
    "            if z > 0:\n",
    "                x = z*(u - cx)/fx\n",
    "                y = z*(v - cy)/fy\n",
    "                point_cloud.append([x, y, z])\n",
    "\n",
    "    point_cloud_rgb = []\n",
    "    # get the rgb class map\n",
    "    class_map = pd.read_csv('../assignment_02/task_2/class_rgb_map.csv').to_dict()\n",
    "\n",
    "    # for each point in the pointcloud, find the nearest point in the semantics using the color intrinsics\n",
    "    for point in point_cloud:\n",
    "        u_rgb = int(point[0]*fx_s/point[2] + cx_s)\n",
    "        v_rgb = int(point[1]*fy_s/point[2] + cy_s)\n",
    "\n",
    "        # get the rgb color the point is within the semantics image\n",
    "        label = int(semantics[v_rgb][u_rgb] * 255)\n",
    "        # create pointcloud in XYZRGB\n",
    "        point_cloud_rgb.append([point[0], point[1], point[2], class_map['r'][label], class_map['g'][label], class_map['b'][label]])\n",
    "\n",
    "    return np.array(point_cloud_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea97d58",
   "metadata": {},
   "source": [
    "# c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "548771f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pointcloud_to_worldframe(pointcloud, pose):\n",
    "    #pass\n",
    "    # make the pointcloud (just XYZ, no RGB) homogenous\n",
    "    pointcloud_h = np.hstack((pointcloud[:, :3], np.ones((pointcloud[:, :3].shape[0], 1))))\n",
    "    # multiply by the pose of the camera\n",
    "    points_worldframe_h = (pose @ pointcloud_h.T)\n",
    "    # transform back from homogenous and transpose\n",
    "    points_worldframe = (points_worldframe_h[:3, :] / points_worldframe_h[3, :]).T\n",
    "    # append the RGB colors again to the transformed points\n",
    "    points_worldframe = np.hstack((points_worldframe, pointcloud[:, 3:]))\n",
    "    return points_worldframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96169916",
   "metadata": {},
   "source": [
    "# d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4d14ef3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_pointcloud(clouds):\n",
    "    #pass\n",
    "    # all pointclouds in the same array and then as a Nx6 vector (XYZRGB)\n",
    "    stitched = []\n",
    "    for cloud in clouds:\n",
    "        stitched.append(cloud)\n",
    "    \n",
    "    stitched = np.vstack(stitched)\n",
    "    return stitched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "11ba3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(stitched_cloud):\n",
    "    #pass\n",
    "    # create the pointcloud object\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    # add points (XYZ) to the point cloud\n",
    "    pcd.points = o3d.utility.Vector3dVector(stitched_cloud[:, :3])\n",
    "    # add the mapped colors to the points\n",
    "    pcd.colors = o3d.utility.Vector3dVector(stitched_cloud[:, 3:6])\n",
    "    # show the pointcloud on screen\n",
    "    o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3286728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"main\":\n",
    "    # Call all your functions here\n",
    "    #pass\n",
    "    # load all data\n",
    "    imgs_c, imgs_d, labels, poses, classes, color_int, depth_int = load_data()\n",
    "    points = []\n",
    "    # iterate over all images\n",
    "    for i in range(len(imgs_c)):\n",
    "        # convert the depth images into colored pointcloud\n",
    "        cloud = convert_depth_semantics_to_colored_pointcloud(imgs_d[i], labels[i], depth_int, color_int)\n",
    "        # transform pointcloud into worldframe\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "        # (needs checking into it, commenting it makes more \"sense\" as everything stays around the same area)\n",
    "        cloud = transform_pointcloud_to_worldframe(cloud, poses[i])\n",
    "        # ---------------------------------------------------------------------------------------------------\n",
    "        # add the points to then stitch them together as a one XYZRGB vector\n",
    "        points.append(cloud)\n",
    "    stitched = stitch_pointcloud(points)\n",
    "    # visualize the result\n",
    "    visualize(stitched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4f9b55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Just to test the code from above\\nimgs_c, imgs_d, labels, poses, classes, color_int, depth_int = load_data()\\npoints = []\\n# iterate over all images\\nfor i in range(len(imgs_c)):\\n    # convert the depth images into colored pointcloud\\n    cloud = convert_depth_semantics_to_colored_pointcloud(imgs_d[i], labels[i], depth_int, color_int)\\n    # transform pointcloud into worldframe\\n    # ---------------------------------------------------------------------------------------------------\\n    # (needs checking into it, commenting it makes more \"sense\" as everything stays around the same area)\\n    cloud = transform_pointcloud_to_worldframe(cloud, poses[i])\\n    # ---------------------------------------------------------------------------------------------------\\n    # add the points to then stitch them together as a one XYZRGB vector\\n    points.append(cloud)\\nstitched = stitch_pointcloud(points)\\n# visualize the result\\nvisualize(stitched)\\n'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Just to test the code from above\n",
    "imgs_c, imgs_d, labels, poses, classes, color_int, depth_int = load_data()\n",
    "points = []\n",
    "# iterate over all images\n",
    "for i in range(len(imgs_c)):\n",
    "    # convert the depth images into colored pointcloud\n",
    "    cloud = convert_depth_semantics_to_colored_pointcloud(imgs_d[i], labels[i], depth_int, color_int)\n",
    "    # transform pointcloud into worldframe\n",
    "    # ---------------------------------------------------------------------------------------------------\n",
    "    # (needs checking into it, commenting it makes more \"sense\" as everything stays around the same area)\n",
    "    cloud = transform_pointcloud_to_worldframe(cloud, poses[i])\n",
    "    # ---------------------------------------------------------------------------------------------------\n",
    "    # add the points to then stitch them together as a one XYZRGB vector\n",
    "    points.append(cloud)\n",
    "stitched = stitch_pointcloud(points)\n",
    "# visualize the result\n",
    "visualize(stitched)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8120c6eb",
   "metadata": {},
   "source": [
    "## 3. 2D TSDF Grid Generation, Weighted Update and Occupancy Computation ##\n",
    "\n",
    "In this task you are provided with a defined parabola given by the equation $y=4x^2$, that represents a curved surface in the first quadrant. The scene is defined over a 1m × 1m area with a  user-defined grid cell size i.e. your method should be general to the size. You are also provided with multiple camera poses that indicate the camera’s position and orientation in the scene. Your goal is to generate a 2D Truncated Signed Distance Function (TSDF) grid by performing raycasting from the camera pose(s) into the scene. The TSDF should be computed as follows: along each ray cast from the camera, determine the intersection with the surface defined by $y=4x^2$ (the measured surface).  For a given grid cell along that ray:\n",
    "\n",
    "- If the cell lies between the camera and the measured surface, assign a positive TSDF value.\n",
    "\n",
    "- If the cell is beyond the measured surface, assign a negative TSDF. \n",
    "\n",
    "- Use TSDF updating principles in the lecture including weight drop-off beyond -0.1m. \n",
    "\n",
    "You can refer to state-of-the-art TSDF weight and distance calculation and update methods in the repository https://github.com/ethz-asl/voxblox. \n",
    "\n",
    "- a.\tGenerate the grid covering the 1m × 1m domain with user defined resolution (Use 0.1m for initial testing). Initialize a maps with two values for the grid: TSDF distance and the weight. Also perform suitable initialization of the weights and distances.     \n",
    "\n",
    "- b.\tUsing the provided camera pose(s), perform raycasting into the scene. You are given the helper function get_intersection_point(). Compute the TSDF value based on the distance from the cell to the measured surface along that ray using the following convention: cells between the camera and the surface have positive TSDF values, and cells beyond the surface have negative TSDF values. Update the cell’s weight only if its absolute TSDF value is within 0.1m. Document your raycasting method and detail how you use the get_intersection_point() function to determine the surface intersection.\n",
    "                                                                                            \t\t\n",
    "- c.\tOnce the TSDF weights and distances are updated for all poses, generate an occupancy map from the two maps.  \n",
    "\t\t\n",
    "- d.\tVisualize the TSDF distance, weights and the occupancy maps for two different gride cell sizes 0.1m and 0.01m.  \n",
    "\t\t\n",
    "- e.\tExplain in text the trade-off between occupancy and TSDF maps on one hand and different grid sizes on the other based on the visualizations generated \n",
    "\n",
    "**Note**: You can add helper functions to make the solution modular. However the functions given below must be completed. You can call your helper functions within it. \n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "80f5abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##############################\n",
    "# Configuration and Constants\n",
    "##############################\n",
    "\n",
    "config = {\n",
    "    'use_weight_dropoff': True,\n",
    "    'default_truncation_distance': 0.1,  # meters\n",
    "    'max_weight': 100.0,\n",
    "    'use_const_weight': False\n",
    "}\n",
    "\n",
    "kFloatEpsilon = 1e-6\n",
    "\n",
    "def get_intersection_point(pose, noise_factor=0.001):\n",
    "    \"\"\"\n",
    "    Compute the intersection point of the sensor ray with the parabola defined by:\n",
    "         y = 4x^2\n",
    "    The sensor ray is given by: (x, y) + t*(cos(theta), sin(theta)).\n",
    "    We solve for t from:\n",
    "         y_cam + t*sin(theta) = 4*(x_cam + t*cos(theta))^2\n",
    "    and add noise proportional to t. The measured distance is returned as negative,\n",
    "    but the intersection point is computed using its absolute value.\n",
    "    \"\"\"\n",
    "    x_cam, y_cam, theta = pose\n",
    "\n",
    "    # Coefficients for the quadratic:\n",
    "    # 4*cos(theta)^2 * t^2 + (8*x_cam*cos(theta) - sin(theta)) * t + (4*x_cam^2 - y_cam) = 0\n",
    "    A = 4 * (np.cos(theta)**2)\n",
    "    B = 8 * x_cam * np.cos(theta) - np.sin(theta)\n",
    "    C = 4 * (x_cam**2) - y_cam\n",
    "\n",
    "    discriminant = B**2 - 4 * A * C\n",
    "    if discriminant < 0:\n",
    "        t_true = 0\n",
    "    else:\n",
    "        t1 = (-B + np.sqrt(discriminant)) / (2 * A)\n",
    "        t2 = (-B - np.sqrt(discriminant)) / (2 * A)\n",
    "        candidates = [t for t in [t1, t2] if t > 0]\n",
    "        t_true = min(candidates) if candidates else 0\n",
    "\n",
    "    noise = np.random.randn() * noise_factor * t_true\n",
    "    t_meas = -(t_true + noise)  # negative measured distance\n",
    "\n",
    "    ray_direction = np.array([np.cos(theta), np.sin(theta)])\n",
    "    # Compute intersection point using absolute measured distance\n",
    "    point_G = np.array([x_cam, y_cam]) + (-t_meas) * ray_direction\n",
    "    return point_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f0531e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_camera_poses_from_csv_file(csv_file):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c0761f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_tsdf_map(grid_size=1.0, cell_size=0.1, config=config):\n",
    "    # Initialize a tsdf map with 2 values each: weight and distance\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4e698b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_tsdf_map(camera_pose,  grid_size=1.0, cell_size=0.2, config=config):\n",
    "    # Update the tsdf map's weight and distances where applicable for the current camera pose\n",
    "    # Use the get_intersection_point() function in above cell to get the intersection point with the parabola which is the surface point\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "dca4e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_occupancy_map(tsdf_map, weight_threshold=0.01,  config=config):\n",
    "    # Compute occupancy map from the tsdf map's weights and distances. If weight < 0.01, assume occupancy is 0.5 i.e. unknown\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2e19d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsdf_weight(tsdf_map):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "06f31055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsdf_dist(tsdf_map):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "82a35217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_occupnacy_map(occupancy_map):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "96f4e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "## Call all the funcs here\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
